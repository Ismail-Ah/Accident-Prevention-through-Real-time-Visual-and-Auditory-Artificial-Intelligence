{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, run this code in the command line to download the required libraries:\n",
    "# pip install opencv-python-headless numpy torch torchvision tensorflow pygame ultralytics depth-anything\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pygame\n",
    "import threading\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24.79M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def parse_arguments(image_path='./assets/examples/demo1.png', depth_vis='./depth_vis', vitl='vits'):\n",
    "    return {'image_path': image_path, 'depth_vis': depth_vis, 'vits': vitl,'grayscale': False,'pred_only':False}\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "margin_width = 50\n",
    "caption_height = 60\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_thickness = 2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(args['vits'])).to(DEVICE).eval()\n",
    "total_params = sum(param.numel() for param in depth_anything.parameters())\n",
    "print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n",
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying depth model on a frame(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_img(frame):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    image = transform({'image': image})['image']\n",
    "    image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        depth = depth_anything(image)\n",
    "\n",
    "    depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
    "\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    depth = depth.cpu().numpy().astype(np.uint8)\n",
    "    return depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-pose.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pygame modules for audio\n",
    "pygame.init()\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Initialize color detection results and sound indexes\n",
    "res = {\"green\": 0, \"red\": 0, \"yellow\": 0}\n",
    "index_color = {\"green\": 2, \"red\": 0, \"yellow\": 1}\n",
    "sounds = {\n",
    "    \"red\": pygame.mixer.Sound(\"sounds/red.wav\"),\n",
    "    \"yellow\": pygame.mixer.Sound(\"sounds/yellow.wav\"),\n",
    "    \"green\": pygame.mixer.Sound(\"sounds/green.wav\")\n",
    "}\n",
    "\n",
    "# Variables to manage sound play timing\n",
    "last_time_played = 0\n",
    "min_delay_between_sounds = [0.7, 0.5, 0.5]\n",
    "\n",
    "image = None\n",
    "\n",
    "# Lock for thread synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Parameters for trapezoidal region\n",
    "width_division = 2\n",
    "height_plus = 100\n",
    "width_diff = 80\n",
    "top_trapew_dist = 50\n",
    "\n",
    "# Yellow color detection for trapezoidal region\n",
    "min_val_yellow = 100\n",
    "max_val_yellow = 170\n",
    "\n",
    "def play_audio(s):\n",
    "    \"\"\"Play sound associated with color if enough time has passed since the last sound.\"\"\"\n",
    "    global last_time_played, last_sound\n",
    "    current_time = time.time()\n",
    "    i = index_color[s]\n",
    "    if current_time - last_time_played > min_delay_between_sounds[i]:\n",
    "        with lock:\n",
    "            sounds[s].play()\n",
    "            last_time_played = current_time\n",
    "\n",
    "def draw_trapez(color, coord):\n",
    "    \"\"\"Draw a semi-transparent trapezoidal overlay on the image.\"\"\"\n",
    "    global image\n",
    "    overlay = image.copy()\n",
    "    vertices = np.array([coord], dtype=np.int32)\n",
    "    cv2.fillPoly(overlay, vertices, color=color)\n",
    "    alpha = 0.3\n",
    "    image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Process each frame to detect colors and manage overlays.\"\"\"\n",
    "    global image\n",
    "    image = frame.copy()\n",
    "    frame = depth_img(frame)\n",
    "    h, w = frame.shape[:2]\n",
    "    w4 = (w // width_division) - top_trapew_dist\n",
    "    w3 = (w // width_division) + top_trapew_dist\n",
    "    h1 = (h // 2) + height_plus\n",
    "    coord_trapezs = [[(w4, h1), (w3, h1), (w - width_diff, h), (width_diff, h)], [], []]\n",
    "    is_red = False\n",
    "    is_yellow = False\n",
    "    for y in range(h1, h):\n",
    "        if is_red:\n",
    "            trapez2draw(is_yellow, is_red, coord_trapezs)\n",
    "            break\n",
    "        x2 = int((y - h1) / (h - h1) * ((w - width_diff) - w3) + w3)\n",
    "        x1 = w - x2\n",
    "        for x in range(x1, x2):\n",
    "            white = frame[y, x]\n",
    "\n",
    "            if white > min_val_yellow:\n",
    "                if not is_yellow and white < max_val_yellow:\n",
    "                    is_yellow = True\n",
    "                    coord_trapezs[0][2], coord_trapezs[0][3] = (x2, y), (x1, y)\n",
    "                    coord_trapezs[1] = [(x1, y), (x2, y), (w - width_diff, h), (width_diff, h)]\n",
    "\n",
    "                if white >= max_val_yellow:\n",
    "                    is_red = True\n",
    "                    if is_yellow:\n",
    "                        coord_trapezs[1][2], coord_trapezs[1][3] = (x2, y), (x1, y)\n",
    "                    else:\n",
    "                        coord_trapezs[0][2], coord_trapezs[0][3] = (x2, y), (x1, y)\n",
    "                    coord_trapezs[2] = [(x1, y), (x2, y), (w - width_diff, h), (width_diff, h)]\n",
    "                    break\n",
    "        if y == h - 1:\n",
    "            trapez2draw(is_yellow, is_red, coord_trapezs)\n",
    "\n",
    "    draw_rect(model, coord_trapezs)\n",
    "\n",
    "def trapez2draw(is_yellow, is_red, coord_trapezs):\n",
    "    \"\"\"Draw appropriate trapezoids based on color detection results.\"\"\"\n",
    "    if is_yellow:\n",
    "        draw_trapez((0, 247, 255), coord_trapezs[1])\n",
    "    if is_red:\n",
    "        draw_trapez((0, 0, 255), coord_trapezs[2])\n",
    "    draw_trapez((0, 255, 0), coord_trapezs[0])\n",
    "\n",
    "def check_intersection(rect, trapez):\n",
    "    \"\"\"Check if a rectangle intersects with a trapezoid.\"\"\"\n",
    "    if trapez:\n",
    "        if rect[1][0] < trapez[3][0] or rect[0][0] > trapez[2][0]:\n",
    "            return False\n",
    "        if rect[1][1] < trapez[0][1] or rect[0][1] > trapez[3][1]:\n",
    "            return False\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def traitement_box(box, coord_trapezs):\n",
    "    \"\"\"Check intersection of detected boxes with trapezoids and update color detection results.\"\"\"\n",
    "    global res, image\n",
    "    x, y, width, height = box.xywh.tolist()[0]\n",
    "    a, b, c, d = int(x - width / 2), int(y - height / 2), int(x + width / 2), int(y + height / 2)\n",
    "    image = cv2.rectangle(image, (a, b), (c, d), (0, 255, 0), 2)\n",
    "    for i in range(2, -1, -1):\n",
    "        if check_intersection([(a, b), (c, d)], coord_trapezs[i]):\n",
    "            if i == 0 and not any(res.values()):\n",
    "                res[\"green\"] = 1\n",
    "            elif i == 1:\n",
    "                res[\"green\"] = 0\n",
    "                if res[\"red\"] == 0:\n",
    "                    res[\"yellow\"] = 1\n",
    "            else:\n",
    "                res = {\"green\": 0, \"red\": 1, \"yellow\": 0}\n",
    "            break\n",
    "\n",
    "def draw_rect(model, coord_trapezs):\n",
    "    \"\"\"Process image with model, handle threading for box processing.\"\"\"\n",
    "    global res, image\n",
    "    info_yolo = model(image)\n",
    "    results = info_yolo[0]\n",
    "    boxes = results.boxes\n",
    "    if boxes:\n",
    "        with threading.Semaphore(len(boxes)): \n",
    "            threads = []\n",
    "            res = {\"green\": 0, \"red\": 0, \"yellow\": 0}\n",
    "            for l in range(len(boxes)):\n",
    "                thread = threading.Thread(target=traitement_box, args=(boxes[l], coord_trapezs))\n",
    "                threads.append(thread)\n",
    "                thread.start()\n",
    "\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "            if res[\"red\"]:\n",
    "                s = \"red\"\n",
    "            elif res[\"yellow\"]:\n",
    "                s = \"yellow\"\n",
    "            elif res[\"green\"]:\n",
    "                s = \"green\"\n",
    "\n",
    "            if any(res.values()):\n",
    "                play_audio(s)\n",
    "\n",
    "# Webcam and display setup\n",
    "try:\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow(\"resultat\")\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        process_frame(frame)\n",
    "        cv2.imshow(\"resultat\", image)\n",
    "        if cv2.waitKey(33) == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
